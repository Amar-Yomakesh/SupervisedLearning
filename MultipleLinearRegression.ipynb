{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc1270c-0cfe-4838-b83a-df49f1ce5609",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression \n",
    "With more than one features x>=2\n",
    "\n",
    "For any prediction there could be more than one deciding factors. For example, Price of the houses not only depend on the area of the house, But also it depends on multiple factors such as, Number of bedrooms, Locality, age of the house and so on. \n",
    "\n",
    "Our task is to design the model which predicts the price of the house appropriately. We use most widely used algorithm, Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951682e3-8fe0-49c4-a830-9d23426d9a19",
   "metadata": {},
   "source": [
    "## 2 Problem Statement\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83766140-5a26-410e-bb86-74ed66295788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2104,    5,    1,   45],\n",
       "        [1416,    3,    2,   40],\n",
       "        [ 852,    2,    1,   35]]),\n",
       " array([460, 232, 178]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x_train = np.array([[2104,5,1,45],[1416,3,2,40],[852,2,1,35]])\n",
    "y_train = np.array([460,232,178])\n",
    "x_train,y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b6dd1b1-50c6-49ec-ba1d-ee98c258bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape is : (3, 4)\n",
      "y_train.shape is : (3,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train.shape is : {x_train.shape}\")\n",
    "print(f\"y_train.shape is : {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c8dc1-ab34-400f-8721-842c53c0955a",
   "metadata": {},
   "source": [
    "x_train is a multi dimentional vector of size (m,n)\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "w (parameter) is vector with single dimension. Because, considering the example each feature will have a single parameter w\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9185fd3b-f3dc-4685-9139-458106fa67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf16ab2-bad6-4a29-8372-4061cb6c421b",
   "metadata": {},
   "source": [
    "### find prediction for 1-d single\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a707c861-4085-4d3e-b682-62e83502e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prediction(x,w,b):\n",
    "    n = x.shape[0]\n",
    "    p = pi = 0 # prediction scalar\n",
    "    for i in range(n):\n",
    "       pi = pi + w[i]*x[i]\n",
    "    p = pi + b\n",
    "    return p\n",
    "def find_prediction_dot(x,w,b):\n",
    "    p = np.dot(w,x) + b\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee022ed-e929-4235-b67a-a3d2e48af5a4",
   "metadata": {},
   "source": [
    "now let's predict for with a single row of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f66406b-a0e5-47bd-8802-eba9003b339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prediction with conventional for loop : 459.9999976194083\n",
      " prediction with dot product : 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "x = x_train[0,:]\n",
    "print(f\" prediction with conventional for loop : {find_prediction(x,w_init,b_init)}\")\n",
    "print(f\" prediction with dot product : {fin_prediction_dot(x,w_init,b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0adcb8-3d87-4057-96c2-0d5f7eb1a80a",
   "metadata": {},
   "source": [
    "#### Now let's compute cost function\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "45daae3a-0605-466b-9e23-3218c40a91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_multi(x,w,b,y,prediction_function):\n",
    "    #x is a vector of multi dimension (m,n) of m rows and n columns -- m example(samples) and n features\n",
    "    #w is a vector of single dimension (,n) -- model parameter vector\n",
    "    #b is a scalar value - base parameter scalar\n",
    "    #y actual value vector (m,)\n",
    "    #c_cost is cost  scalar\n",
    "    c_cost = 0\n",
    "    m = x.shape[0]\n",
    "    for i in range(m):\n",
    "        p = prediction_function(x[i],w,b)\n",
    "        c_cost = c_cost + ((p - y[i])**2)\n",
    "    c_cost = c_cost / (2 * m)                      #scalar  \n",
    "    return c_cost\n",
    "def compute_cost(x,y,w,b):\n",
    "    m =x.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        cost = cost + (np.dot(x[i],w) - y[i])**2\n",
    "    cost = cost/2*m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbac8046-4d19-4d87-9a75-672eaec88142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5578904045996674e-12"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost_multi(x_train,w_init,b_init,y_train,find_prediction_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e70d5-93a4-4223-88b3-e1d7b3268b67",
   "metadata": {},
   "source": [
    "## Gradient descent with multiple variables\n",
    "Above we calculated the cost for a w = [$w_{0}$ , $w_{1}$ , $w_{3}$ ... $w_{n-1}$]\n",
    "\n",
    "Idea of gradient descent is to arrive at miniumum value of cost for w and b\n",
    "final output we are looking to arrive at is $w_{m,n}$ vector with m samples and n features. and b scalar value.\n",
    "initialise w_init = [$w_{0}$ , $w_{1}$ , $w_{3}$ ... $w_{n-1}$] and b_init = 0\n",
    "and with learning factor $\\alpha$\n",
    "\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e87f566-e2a6-4947-a26d-5e9bd607d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_multi_var(x,y,w,b):\n",
    "    m,n = x_train.shape\n",
    "    err = 0\n",
    "    dj_w = np.zeros((n,))\n",
    "    dj_b = 0\n",
    "    for i in range(m):\n",
    "        err = (np.dot(x[i],w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_w[j] = dj_w[j] + err * x[i,j]\n",
    "        dj_b = dj_b + err\n",
    "    return dj_w/m , dj_b/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56144853-9241-43d2-be27-279b6a1c466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients are dj_w : [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05] and dj_b = -1.6739251122999121e-06\n"
     ]
    }
   ],
   "source": [
    "tmp_dj_dw, tmp_dj_db  = compute_gradients_multi_var(x_train, y_train, w_init, b_init)\n",
    "print(f\"gradients are dj_w : {tmp_dj_dw} and dj_b = {tmp_dj_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f56073-1539-4031-bead-7af09ac1e448",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1f991463-1ea4-4396-bf23-7a50d334d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def compute_gradient_descent(X,y,w_in,b_in,gradient_funtion,cost_function,alpha,iterations):\n",
    "    j_Hist = [] # the list will be of length of number of iterations carried\n",
    "    initial_w = np.zeros_like(w_init)\n",
    "    w = copy.deepcopy(initial_w)\n",
    "    b = b_in\n",
    "    for i in range(iterations):\n",
    "        #print(f\"this is {i}th loop\")\n",
    "        grdadient = gradient_funtion(X,y,w,b)\n",
    "        w = w - (alpha * (grdadient)[0])\n",
    "        b = b - (alpha * (grdadient)[1])\n",
    "        if i < 10000:\n",
    "           j = cost_function(X,y,w,b)\n",
    "           j_Hist.append(j)\n",
    "        if i % 10 == 0:\n",
    "            #print(j_Hist)\n",
    "            pass\n",
    "    return w,b,j_Hist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0eeb2ae4-0e5e-48d0-9cc0-ab6140a5c99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final w found by the model is : [ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] final b found by the model is -0.0022354075309325345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0.39133535,  18.75376741, -53.36032453, -26.42131618])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_init = 0.\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "w_final, b_final, j_final = compute_gradient_descent(x_train,y_train,w_init,b_init,compute_gradients_multi_var,compute_cost,alpha,iterations)\n",
    "print(f\"final w found by the model is : {w_final} final b found by the model is {b_final}\")\n",
    "w_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "469fc5b3-dffa-4483-91a4-6783cc4ab980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 % 10 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2b7c244c-dea3-468f-91a6-abe8e33ac787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([460, 232, 178])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a34aea17-8afc-4f1c-ac5f-69ab5c64c756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.39133535,  18.75376741, -53.36032453, -26.42131618])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "df564da8-8b01-46d9-ad8f-3914febb4b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785.1811367994083"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3953e0e1-c94d-4c9a-ba4d-8cc20fcdb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5.0e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f3e36a79-3c08-4fc6-80c6-deb13aa896c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9999999999999996e-06"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f3b18-6bb5-46f8-be0b-0e9f49b12d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
